# Question answering System for Tunisian Governement websites


## Setup
### Virtual Environement
```SHELL
conda create -n gov-qa-env python=3.9
conda activate gov-qa-env
```
### Dependencies
```SHELL
pip install -r requirements.txt
```
## Objective
The goal is to build a QA answering system that's composed of two parts: the retriever and the question answering model. In the following section, we explore different approaches that are variations of the metioned structure and see which one gives the best result and has faster performance.

## Dataset
The data is scraped from all gov .tn websites using scrapy spiders.

## Approaches

### Approach 1 (Information Retrieval + Reading comprehension QA)
- Scrape all the textual data from Governement websites to build the textual knowledge corpus
    - Crawl every page and retrieve textual data
    - Preprocess data
        - Remove stop words
        - Stemming/Lemmatization
    - Seperating the data into documents
    - Generate a TFIDF matrix from documents
- Use TFIDF for information retrieval
    - The user inputs a query and its dot product with TFIDF returns top K most relevant documents
- Use finetuned transformer model (camemBERT) for Reading Comprehension
    - Model extracts answer passage from each top K relevant document
    - Answers are ranked
    
### TO DO
- [ ] Scrape all gov websites
- [ ] Scrape the entirety of each website 
    - [ ] Figure out a way to generalize crawling of websites to get all paragraphs instead of doing it case by case (if possible) 
- [ ] Figure out a policy to seperate scraped data into documents
- [ ] Use top k most relevant documents and rank answers by model confidence scores
- [ ] Use embedding similarity instead of TF-IDF for retrieval
- [ ] Return the source with the answer (paragraph and segment from which it was retrieved)
- [ ] Use an LLM for QA (optimized for CPU/ distilled/ quantization ...)
- [ ] Experiment with arabic LMs (for QA) and embeddings (for retrieval based on similarity) 
- [ ] Implement REALM

### Approach 2
- using **REALM**: Retrieval Augmented Language Model Pre-Training
- In this case the retriever is also learnable
- The idea is for a LLM to rely on a textual knowledge source instead of "encoding" all the information from the corpus in its parameters (which means, more knowledge would require bigger neural networks)


### Approach 3: 
- Approach: use open generative Question Answering
- The system is only given the query as input and generates the answer 


## Question answering models
This is an overview of how the question answering downstream task is categorized in NLP.

- **Extractive QA**: model provides answer given a corpus of text (i.e. context). When provided with a question, the model searches through the documents to pinpoint the best answer to that question
- **Open Generative QA**: The model generates text based on context. The answer does **not** have to be in the answer
- **Closed Generative QA**: No context is provided and the answer is generated by the model



## Arabic Large Language Models 
These are some of the arabic LLMS that we consider using in case we want the system to work in arabic.

| Model Name | Description |
| ---------- | ----------- |
| mBERT | A pre-trained language model that supports 104 languages, including Arabic, and can be fine-tuned for various NLP tasks, including question answering. |
| AraBERT | A pre-trained language model specifically designed for Arabic text. It was trained on a large corpus of Arabic text and achieved state-of-the-art results on several Arabic NLP tasks, including question answering. |
| MADAR | A collection of Arabic language models that are specifically designed for various Arabic dialects. The models were trained on a large corpus of Arabic text and can be fine-tuned for various NLP tasks, including question answering. |
| Gulf Arabic BERT | A pre-trained language model that is specifically designed for the Gulf Arabic dialect. It was trained on a large corpus of Gulf Arabic text and achieved state-of-the-art results on several Arabic NLP tasks, including question answering. |
| AraGPT2 | A pre-trained language model that is specifically designed for Arabic text. It was trained on a large corpus of Arabic text and can be fine-tuned for various NLP tasks, including question answering. |


Urls: https://docs.google.com/spreadsheets/d/1xMcJJQ7s7S4zlU5HByVRHe9B2D9KF_cU7aYIHnLcqrY/edit#gid=0
