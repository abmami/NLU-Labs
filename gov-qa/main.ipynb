{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "from transformers import pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the camembert model and its tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline('question-answering', model='etalab-ia/camembert-base-squadFR-fquad-piaf', tokenizer='etalab-ia/camembert-base-squadFR-fquad-piaf')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('french'))\n",
    "stemmer = SnowballStemmer(('french'))\n",
    "punctuations = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tokens(words):\n",
    "    \"\"\"\" Processes each word of a given list\n",
    "    Args:\n",
    "        words: list of words to be processed\n",
    "    Returns: list\n",
    "    \"\"\"\n",
    "    tokens = [w for w in words if not w.lower() in stop_words and w.lower() not in punctuations]\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    tokens = [t for t in  tokens if any(c.isnumeric() for c in t)==False]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def process_document(document):\n",
    "    \"\"\"Processes a string of text\n",
    "    Args:\n",
    "        document: string to be processed\n",
    "    Returns: processed string\n",
    "    \"\"\"\n",
    "    words = document.split(' ')\n",
    "    words = process_tokens(words)\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "\n",
    "def get_documents(json_file):\n",
    "    \"\"\"Retrieves all scraped documents\n",
    "    Arguments:\n",
    "        json_file: json file containting scraped data\n",
    "    Returns: list of documents\n",
    "    \"\"\"\n",
    "    with open(f'./data/{json_file}', encoding='UTF-8') as f:\n",
    "        data = json.load(f)\n",
    "        corpus = [item['content'] for item in data]\n",
    "        return corpus\n",
    "        \n",
    "\n",
    "\n",
    "def get_relevant_doc(query, documents):\n",
    "    \"\"\"Retrives most relevant document from list of documents using TFIDF\n",
    "    Args:\n",
    "        query: input query from the user\n",
    "        documents: list of scraped documents\n",
    "    Returns: most relevant doc (highest cosine similarity) \n",
    "    \"\"\"\n",
    "    processed_documents = [process_document(d) for d in documents]\n",
    "    query = process_document(query)\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(processed_documents)\n",
    "    query_vector = vectorizer.transform([query])\n",
    "\n",
    "    cosine_similarities = np.dot(tfidf_matrix, query_vector.T)\n",
    "    best_document_index = cosine_similarities.argmax()\n",
    "    best_document = documents[best_document_index]\n",
    "    return best_document"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = get_documents('finance_glob.json')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting query from user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'que fait le ministère de finances?'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetchig most relevant document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = get_relevant_doc(query, documents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using camembert for Reading Comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " assure la gestion du Trésor public.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result = nlp({\n",
    "'question': query,\n",
    "'context': doc\n",
    "})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-117",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
