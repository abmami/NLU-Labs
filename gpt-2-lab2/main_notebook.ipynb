{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import time\n",
    "from datetime import timedelta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data and applying character-level tokenziation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "\n",
    "def encode(s): return [stoi[c] for c in s]\n",
    "def decode(l): return ''.join([itos[i] for i in l])\n",
    "\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Hyperparmeters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x211d69b2b50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "block_size = 256\n",
    "max_iters = 1000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embed = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "\n",
    "torch.manual_seed(1337)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split: str) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Generates a batch of training or validation data for a language modeling task.\n",
    "\n",
    "    Args:\n",
    "    - split (str): The split to use for the data ('train' or 'val')\n",
    "\n",
    "    Returns:\n",
    "    - x (torch.Tensor): The input tensor with shape (batch_size, block_size)\n",
    "    - y (torch.Tensor): The target tensor with shape (batch_size, block_size)\n",
    "    \"\"\"\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    \"\"\"Estimates the loss of a given model on the train and validation datasets.\n",
    "    Returns:\n",
    "    out : dict\n",
    "        A dictionary containing the average loss for the train and validation datasets.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT language model components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"\n",
    "    One head of self-attention.\n",
    "    \n",
    "    Parameters:\n",
    "        head_size (int): The size of the head.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "\n",
    "        # tril: lower triangle matrix or masking (it's not parameter and gets assigned the following way)\n",
    "        self.register_buffer('tril', torch.tril(\n",
    "            torch.ones(block_size, block_size)))\n",
    "    \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Parameters:\n",
    "            x (torch.Tensor): The input tensor of shape `(batch_size, seq_len, embed_dim)`.\n",
    "        \n",
    "        Returns:\n",
    "            out (torch.Tensor): The output tensor of shape `(batch_size, seq_len, embed_dim)`.\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        # Attention scores\n",
    "        # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # Weighted agregation of the values\n",
    "        v = self.value(x)  # (B, T, C)\n",
    "        out = wei @ v  # (B, T, T) @ (B, T, C) -> (B, T ,C)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAtttention(nn.Module):\n",
    "    \"\"\"Multiple heads of self-attenton in parallel.\n",
    "\n",
    "    Args:\n",
    "        num_heads (int): The number of parallel self-attention heads.\n",
    "        head_size (int): The size of each self-attention head.\n",
    "    \n",
    "    Attributes:\n",
    "        heads (nn.ModuleList): The list of self-attention heads.\n",
    "        proj (nn.Linear): The linear projection layer that transforms the concatenated\n",
    "            self-attention head outputs to the original embedding size.\n",
    "        dropout (nn.Dropout): The dropout layer to apply after the projection.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass for the multi-head self-attention.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor of shape `(batch_size, seq_len, n_embed)`.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor of shape `(batch_size, seq_len, n_embed)`.\n",
    "        \"\"\"\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Applies a simple feedforward network to the input.\n",
    "\n",
    "    Args:\n",
    "        n_embed (int): The size of the input embedding.\n",
    "\n",
    "    Attributes:\n",
    "        net (nn.Sequential): The feedforward network consisting of two linear layers with ReLU activation\n",
    "            and a final dropout layer.\n",
    "\n",
    "    Methods:\n",
    "        forward(x): Applies the feedforward network to the input tensor x.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embed, n_embed),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, n_embed)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, seq_len, n_embed)\n",
    "        \"\"\"\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    A Transformer block that performs self-attention and feedforward computation.\n",
    "\n",
    "    Args:\n",
    "    - n_embed (int): the size of the input vector\n",
    "    - n_head (int): the number of attention heads to use\n",
    "\n",
    "    Attributes:\n",
    "    - sa (MultiHeadAtttention): a MultiHeadAtttention layer that performs self-attention\n",
    "    - ffwd (FeedForward): a FeedForward layer that applies a non-linearity after a linear transformation\n",
    "    - ln1 (nn.LayerNorm): a LayerNorm layer that normalizes the input after self-attention\n",
    "    - ln2 (nn.LayerNorm): a LayerNorm layer that normalizes the input after the feedforward layer\n",
    "\n",
    "    Methods:\n",
    "    - forward(x): performs a forward pass through the block, taking in an input tensor x of shape (batch_size, seq_len, n_embed)\n",
    "                  and returning an output tensor of the same shape after applying self-attention and feedforward computation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        # We cut the input vector of size n_embed into n_head chunks of head_size\n",
    "        head_size = n_embed // n_head\n",
    "        self.sa = MultiHeadAtttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        self.ln1 =nn.LayerNorm(n_embed)\n",
    "        self.ln2 =nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A Transformer-based language model for predicting the next token in a sequence.\n",
    "\n",
    "    Parameters:\n",
    "        vocab_size (int): The size of the vocabulary.\n",
    "        n_embed (int): The dimensionality of the token and position embeddings.\n",
    "        block_size (int): The length of the input sequence.\n",
    "        n_head (int): The number of attention heads in each Transformer block.\n",
    "        n_layer (int): The number of Transformer blocks.\n",
    "    \n",
    "    Attributes:\n",
    "        token_embedding_table (nn.Embedding): The embedding layer for the input tokens.\n",
    "        position_embedding_table (nn.Embedding): The embedding layer for the position of each token.\n",
    "        blocks (nn.Sequential): A sequence of Transformer blocks.\n",
    "        ln_f (nn.LayerNorm): A layer normalization layer.\n",
    "        lm_head (nn.Linear): A linear layer for predicting the next token.\n",
    "\n",
    "    Methods:\n",
    "        forward(idx, targets=None): Computes the forward pass of the model.\n",
    "        generate(idx, max_new_tokens): Generates new tokens given an input sequence.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "\n",
    "    \n",
    "\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(\n",
    "            torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond) # Predictions\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instanciating the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel()\n",
    "m = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10786622 parameters\n"
     ]
    }
   ],
   "source": [
    "print(f'{sum(p.numel() for p in m.parameters())} parameters')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model on \"The Prophet\" by Gibran Khalil Gibran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: cuda\n",
      "\n",
      "step 0: train loss 2.0388, val los 2.1084\n",
      "step 500: train loss 1.2227, val los 1.5842\n",
      "step 999: train loss 0.6575, val los 1.7365\n",
      "Training Time: 0:23:16.088192\n"
     ]
    }
   ],
   "source": [
    "print(f'Training on: {device}\\n')\n",
    "\n",
    "start = time.time()\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f'step {iter}: train loss {losses[\"train\"]:.4f}, val los {losses[\"val\"]:.4f}')\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "end = time.time()\n",
    "print('Training Time:', str(timedelta(seconds=end-start)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "And he timplactes of your be?\n",
      "\n",
      "Let the reforsectles ameme ints, the flam a your emeter beling foreed.\n",
      "\n",
      "And the truth trel theivals of your beling.\n",
      "\n",
      "When a pastroud washion you are not though you an eclose?\n",
      "\n",
      "The houre are my flies the wind their unmostan And thicheld stand dired your dreamless priesures, spicenc.\n",
      "\n",
      "Amin eterns with them your own he his heags in hilking in of beceieuiled.\n",
      "\n",
      "For in hiddenstreat:\n",
      "\n",
      "It is not though worde of murderen,\n",
      "\n",
      "In the man the vals of him said, only bealdshing?”\n",
      "\n",
      "But my of the own dreath eas the treasurembling in the beclow, togethere whiler too near ocein.\n",
      "\n",
      "And heldef, its visideded, he on her them endersty is in morn the light, our more and mememboraders:\n",
      "\n",
      "Then of others night, and of too crier shad you under them like?\n",
      "\n",
      "I come of the streast, and the wineppiness, and he who spirit our found the kill which all that still of lize the naught is off hidden.\n",
      "\n",
      "For if is it elutenill give, in the lifthe is that ber own day aring;\n",
      "\n",
      "Too veers and those infore\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
